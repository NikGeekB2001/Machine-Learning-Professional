{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b542b465",
      "metadata": {
        "id": "b542b465"
      },
      "source": [
        "<h6 style= 'color: blue'>  Часть вторая </h6>  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0d3fc325",
      "metadata": {
        "id": "0d3fc325"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Импортируем библиотеку numpy для работы с линейной алгеброй\n",
        "import numpy as np\n",
        "# Импортируем библиотеку pandas для работы с данными, чтения и записи файлов в формате CSV\n",
        "import pandas as pd\n",
        "# Импортируем модуль warnings для управления предупреждениями, которые могут возникать при выполнении кода\n",
        "import warnings\n",
        "# Используем метод filterwarnings для игнорирования всех предупреждений\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Импортируем модуль sklearn.model_selection для разделения данных на обучающую и тестовую выборки\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Импортируем модуль sklearn.metrics для оценки качества моделей машинного обучения\n",
        "from sklearn.metrics import classification_report\n",
        "# Импортируем модуль sklearn.preprocessing для стандартизации данных\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Импортируем модуль matplotlib для работы с графикой\n",
        "import matplotlib as plt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a324de2",
      "metadata": {
        "id": "8a324de2"
      },
      "source": [
        "# Поиск оптимальных параметров для моделей\n",
        "\n",
        "Посмотрим библиотеку Optuna, которая предназначена для автоматического подбора оптимальных гиперпараметров для различных моделей машинного обучения и библиотеку PyOD, ее основные функции, примеры использования и сравнение производительности различных моделей детекции выбросов.\n",
        "Мы хотим найти оптимальные параметры для некоторых моделей детекции выбросов из библиотеки PyOD, чтобы позже построить их с оптимальными параметрами, проанализировать результаты и добиться максимальной интегральной точности с их помощью. Это интересная и сложная задача.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8c0467d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "8c0467d5",
        "outputId": "2dd2e3cb-6fc1-4f1e-9cb2-02bf8bdaceb3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4203f1ac3501>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Файл CSV содержит информацию о транзакциях с кредитных карт, которые были совершены в течение двух дней в сентябре 2013 года в Европе\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# В файле есть 284807 строк и 31 столбец\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'creditcard.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Удаляем столбцы Time и Class из датафрейма и сохраняем оставшиеся столбцы в датафрейме X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Столбец Time показывает время транзакции в секундах от первой транзакции в датасете, но он не влияет на то, является ли транзакция мошеннической или нет\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'creditcard.csv'"
          ]
        }
      ],
      "source": [
        "\n",
        "## Сначало загружаем данные\n",
        "\n",
        "# Читаем данные из файла CSV и сохраняем их в датафрейме df\n",
        "# Файл CSV содержит информацию о транзакциях с кредитных карт, которые были совершены в течение двух дней в сентябре 2013 года в Европе\n",
        "# В файле есть 284807 строк и 31 столбец\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "# Удаляем столбцы Time и Class из датафрейма и сохраняем оставшиеся столбцы в датафрейме X\n",
        "# Столбец Time показывает время транзакции в секундах от первой транзакции в датасете, но он не влияет на то, является ли транзакция мошеннической или нет\n",
        "# Столбец Class показывает метку класса для транзакции (0 - не мошенническая, 1 - мошенническая), но он не является признаком, а является целевой переменной, которую мы хотим предсказать\n",
        "X = df.drop(columns=['Time', 'Class'])\n",
        "# Сохраняем столбец Class в серии y, которая будет содержать метки классов для транзакций\n",
        "y = df.Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c2dcba",
      "metadata": {
        "id": "d9c2dcba"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Просматриваем\n",
        "df.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8647cb69",
      "metadata": {
        "id": "8647cb69"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Просматриваем\n",
        "y.head (5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb3a578",
      "metadata": {
        "id": "4eb3a578"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Просматриваем\n",
        "X.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7934a1",
      "metadata": {
        "id": "2c7934a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Импортируем класс StandardScaler из модуля sklearn.preprocessing для стандартизации данных\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Создаем объект norm, который будет представлять стандартизатор данных\n",
        "norm = StandardScaler()\n",
        "# Применяем метод fit_transform к датафрейму X, который содержит признаки транзакций, и получаем массив с нормализованными данными\n",
        "# Метод fit_transform сначала вычисляет среднее и стандартное отклонение для каждого столбца в датафрейме, а затем преобразует каждое значение в столбце по формуле: (значение - среднее) / стандартное отклонение\n",
        "# Таким образом, мы получаем данные, в которых каждый столбец имеет среднее равное нулю и стандартное отклонение равное единице\n",
        "X_norm = pd.DataFrame(norm.fit_transform(X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b749dcc4",
      "metadata": {
        "id": "b749dcc4"
      },
      "source": [
        "\n",
        " Ниже, мы хотим определить функцию, которая возвращает часть данных о транзакциях с кредитных карт, которую можем использовать для обучения моделей детекции выбросов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999b0fcc",
      "metadata": {
        "id": "999b0fcc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Определяем функцию get_data_part, которая принимает на вход датафрейм X с признаками транзакций, серию y с метками классов и параметр size, который задает долю данных, которую мы хотим взять\n",
        "def get_data_part(X, y, size=0.1):\n",
        "    # Импортируем функцию train_test_split из модуля sklearn.model_selection для разделения данных на обучающую и тестовую выборки\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Применяем функцию train_test_split к датафрейму X и серии y, чтобы получить четыре объекта: X_train, X_test, y_train, y_test\n",
        "    # Параметр random_state=0 означает, что мы фиксируем случайное состояние разделения для воспроизводимости результатов\n",
        "    # Параметр train_size=size означает, что мы берем size процентов данных для обучающей выборки, а остальные данные для тестовой выборки\n",
        "    # Параметр shuffle=True означает, что мы перемешиваем данные перед разделением, чтобы избежать смещения\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, random_state=0, train_size=size, shuffle=True\n",
        "    )\n",
        "    # Выводим на экран размер обучающей выборки, который равен количеству строк в датафрейме X_train\n",
        "    print('train size: {}'.format(X_train.shape[0]))\n",
        "    # Вычисляем и выводим на экран долю мошеннических транзакций в обучающей выборке, которая называется загрязнением (contamination)\n",
        "    # Для этого используем метод value_counts с параметром normalize=True, который возвращает частоту каждого уникального значения в серии y_train\n",
        "    # Затем выбираем значение с индексом 1, которое соответствует мошенническим транзакциям, и округляем его до трех знаков после запятой\n",
        "    contamination = round(y_train.value_counts(normalize=True)[1], 3)\n",
        "    # Выводим на экран значение загрязнения в процентах\n",
        "    print('contamination: {} ({}%)'.format(contamination, contamination * 100))\n",
        "    # Возвращаем из функции три объекта: X_train, y_train и contamination\n",
        "    return X_train, y_train, contamination\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75e5d281",
      "metadata": {
        "id": "75e5d281"
      },
      "outputs": [],
      "source": [
        "\n",
        "  # Вызываем функцию get_data_part с датафреймом X_norm, который содержит стандартизованные признаки транзакций, серией y,\n",
        "# которая содержит метки классов, и параметром size, который задает долю данных, которую мы хотим взять\n",
        "# Параметр size можно изменить в зависимости от того, сколько данных мы хотиm использовать для обучения моделей\n",
        "# Сохраняем результаты функции в трех переменных: X_train, y_train и contamination\n",
        "X_train, y_train, contamination = get_data_part(X_norm, y, size=0.1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5579813b",
      "metadata": {
        "id": "5579813b"
      },
      "outputs": [],
      "source": [
        "contamination # доля мошеннических транзакций в обучающей выборке равна 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf739db6",
      "metadata": {
        "id": "bf739db6"
      },
      "outputs": [],
      "source": [
        "# Метки классов для тестовой выборки\n",
        "# y_train это переменная, которая содержит зависимые переменные, называемые целевыми, для обучения модели машинного обучения.\n",
        "# Это то, что вы пытаетесь предсказать или объяснить с помощью вашей модели\n",
        "y_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56af79e2",
      "metadata": {
        "id": "56af79e2"
      },
      "source": [
        "> y_train - это серия, которая содержит метки классов для транзакций, которые были выбраны для обучающей выборки. Метки классов показывают, является ли транзакция мошеннической или нет. В серии y_train есть два возможных значения: 0 - не мошенническая, 1 - мошенническая. Серия y_train имеет тот же размер и индексы, что и датафрейм X_train, который содержит признаки транзакций для обучающей выборки. Серия y_train была получена с помощью функции get_data_part, которую мы определили ранее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16006851",
      "metadata": {
        "id": "16006851"
      },
      "outputs": [],
      "source": [
        "type(y_train) # Объект Серия"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73373a3c",
      "metadata": {
        "id": "73373a3c"
      },
      "outputs": [],
      "source": [
        "# Просмотр\n",
        "y_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a6cbe5",
      "metadata": {
        "id": "15a6cbe5"
      },
      "source": [
        "> Можно сделать следующие выводы:\n",
        "\n",
        "Количество наблюдений в Series равно 28480.\n",
        "Среднее значение переменной Class равно 0.001791, что означает, что доля мошеннических транзакций в Series очень мала.\n",
        "Стандартное отклонение переменной Class равно 0.042280, что означает, что значения переменной Class сильно разбросаны вокруг среднего.\n",
        "Минимальное и максимальное значение переменной Class равны 0 и 1 соответственно, что подтверждает, что это бинарный признак.\n",
        "25%, 50% и 75% квантили переменной Class равны 0, что означает, что большинство транзакций в Series являются нормальными. Только 25% транзакций имеют значение переменной Class равное 1, что означает, что они являются мошенническими."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00a2636",
      "metadata": {
        "id": "b00a2636"
      },
      "outputs": [],
      "source": [
        "# Выводим на экран первые пять строк датафрейма X_train, который содержит признаки транзакций для обучающей выборки\n",
        "X_train.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fa5a50",
      "metadata": {
        "id": "d0fa5a50"
      },
      "source": [
        "## Модели PyOD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4733694",
      "metadata": {
        "id": "d4733694"
      },
      "outputs": [],
      "source": [
        "%pip install pyod\n",
        "# Я пока закоментирую."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69cea48",
      "metadata": {
        "id": "b69cea48"
      },
      "outputs": [],
      "source": [
        "# Импортируем функцию evaluate_print из модуля pyod.utils.data, который содержит полезные утилиты для работы с данными и моделями детекции выбросов\n",
        "from pyod.utils.data import evaluate_print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88235d26",
      "metadata": {
        "id": "88235d26"
      },
      "outputs": [],
      "source": [
        "%pip install optuna\n",
        "# Я пока закоментирую."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29853c91",
      "metadata": {
        "id": "29853c91"
      },
      "outputs": [],
      "source": [
        "# Импортируем библиотеку Optuna, которая предназначена для автоматического подбора оптимальных гиперпараметров\n",
        "# для различных моделей машинного обучения\n",
        "import optuna\n",
        "\n",
        "# Импортируем функции plot_param_importances и plot_parallel_coordinate из модуля optuna.visualization.matplotlib,\n",
        "# которые позволяют визуализировать важность параметров и параллельные координаты с помощью библиотеки matplotlib\n",
        "from optuna.visualization.matplotlib import plot_param_importances, plot_parallel_coordinate\n",
        "\n",
        "# Импортируем функцию average_precision_score из модуля sklearn.metrics, которая вычисляет среднюю точность\n",
        "# (average precision, AP) из прогнозных оценок\n",
        "# AP является одной из метрик для оценки качества детекции выбросов (лайфхак)\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# Импортируем функцию confusion_matrix из модуля sklearn.metrics, которая вычисляет матрицу ошибок (confusion matrix)\n",
        "# из истинных и предсказанных меток классов\n",
        "# Матрица ошибок является другой метрикой для оценки качества детекции выбросов\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Импортируем класс ConfusionMatrixDisplay из модуля sklearn.metrics, который позволяет визуализировать матрицу\n",
        "# ошибок с помощью библиотеки matplotlib\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc379055",
      "metadata": {
        "id": "bc379055"
      },
      "outputs": [],
      "source": [
        "# Определяем функцию optimize, которая принимает на вход функцию objective, которая задает целевую функцию для оптимизации,\n",
        "# и параметр n, который задает количество испытаний для оптимизации\n",
        "def optimize(objective, n):\n",
        "    # Импортируем библиотеку Optuna для автоматического подбора гиперпараметров\n",
        "    import optuna\n",
        "    # Создаем объект study, который будет представлять процесс оптимизации\n",
        "    # Параметр direction='maximize' означает, что мы хотим максимизировать значение целевой функции\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "\n",
        "    # Запускаем оптимизацию с помощью метода optimize, который принимает на вход функцию objective, параметр n_trials,\n",
        "    # который задает количество испытаний для оптимизации, и параметр show_progress_bar, который показывает индикатор прогресса\n",
        "    study.optimize(objective, n_trials=n, show_progress_bar=True)\n",
        "\n",
        "    # Выводим на экран лучшие параметры и лучшее значение целевой функции, которые были найдены в процессе оптимизации\n",
        "    print('\\nBest params:{}\\nBest value:{}'.format(study.best_params, study.best_value))\n",
        "\n",
        "    # Импортируем функцию plot_param_importances из модуля optuna.visualization.matplotlib, которая позволяет визуализировать\n",
        "    # важность параметров с помощью библиотеки matplotlib\n",
        "    from optuna.visualization.matplotlib import plot_param_importances\n",
        "\n",
        "    # Строим график важности параметров с помощью функции plot_param_importances, которая принимает на вход объект study\n",
        "    plot_param_importances(study)\n",
        "\n",
        "    # Импортируем функцию plot_parallel_coordinate из модуля optuna.visualization.matplotlib, которая позволяет визуализировать\n",
        "    # параллельные координаты с помощью библиотеки matplotlib\n",
        "\n",
        "    from optuna.visualization.matplotlib import plot_parallel_coordinate\n",
        "    # Строим график параллельных координат с помощью функции plot_parallel_coordinate, которая принимает на вход объект study\n",
        "    plot_parallel_coordinate(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b64abd",
      "metadata": {
        "id": "01b64abd"
      },
      "outputs": [],
      "source": [
        "# Определяем функцию metrics, которая принимает на вход модель кластеризации и истинные метки классов\n",
        "def metrics(model, y_train):\n",
        "    # Устанавливаем случайное зерно для воспроизводимости результатов\n",
        "    np.random.seed(42)\n",
        "    # Предсказываем метки классов с помощью модели\n",
        "    y_pred = model.labels_\n",
        "    # Выводим на экран название модели и метрики качества кластеризации: гомогенность, полноту и V-меру\n",
        "    evaluate_print(model.__class__, y_train, y_pred)\n",
        "    # Выводим на экран отчет о классификации, который содержит точность, полноту, F-меру и поддержку для каждого класса\n",
        "    print(classification_report(y_train, y_pred))\n",
        "    # Вычисляем матрицу ошибок, которая показывает, сколько объектов каждого класса было правильно или неправильно отнесено к другому классу\n",
        "    cm = confusion_matrix(y_train, y_pred)\n",
        "    # Выводим на экран матрицу ошибок\n",
        "    print(cm)\n",
        "    # Создаем объект для визуализации матрицы ошибок с помощью библиотеки matplotlib\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    # Строим график матрицы ошибок\n",
        "    disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3e1a54",
      "metadata": {
        "id": "6e3e1a54"
      },
      "outputs": [],
      "source": [
        "# Устанавливаем случайное зерно для воспроизводимости результатов\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11352452",
      "metadata": {
        "id": "11352452"
      },
      "source": [
        "### kNN Example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c4f9806",
      "metadata": {
        "id": "2c4f9806"
      },
      "source": [
        " > kNN - это алгоритм машинного обучения, который используется для решения задач классификации и регрессии. Он основан на идее, что объекты с похожими признаками имеют похожие метки классов или значения. kNN работает так: для нового объекта он находит k ближайших соседей в обучающей выборке, то есть те объекты, которые имеют наименьшее расстояние до нового объекта. Затем он присваивает новому объекту метку класса или значение, которое является наиболее частым среди его соседей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1022a76a",
      "metadata": {
        "id": "1022a76a"
      },
      "outputs": [],
      "source": [
        "from pyod.models.knn import KNN\n",
        "\n",
        "# Импортируем библиотеку pyod, которая предоставляет различные алгоритмы для обнаружения аномалий в данных\n",
        "from pyod.models.knn import KNN\n",
        "# Импортируем класс KNN из модуля pyod.models.knn, который реализует алгоритм kNN для обнаружения аномалий\n",
        "# Алгоритм kNN для обнаружения аномалий работает так:\n",
        "# для каждого объекта он находит k ближайших соседей в обучающей выборке, то есть те объекты, которые имеют\n",
        "# наименьшее расстояние до данного объекта. Затем он вычисляет аномальный коэффициент для каждого объекта как\n",
        "# среднее расстояние до его соседей. Чем больше аномальный коэффициент, тем больше вероятность, что объект является аномалией."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56adc9b0",
      "metadata": {
        "id": "56adc9b0"
      },
      "outputs": [],
      "source": [
        "# Мы вызываем функцию get_data_part, которая принимает три аргумента:\n",
        "# X_norm, y и size.\n",
        "# X_norm - это массив, который содержит нормализованные значения признаков из исходных данных,\n",
        "# y - это массив, который содержит метки классов для каждого объекта из исходных данных,\n",
        "# а size - это размер выборки, которую мы хотим получить из исходных данных.\n",
        "# Функция get_data_part возвращает три значения:\n",
        "# X_train, y_train и contamination.\n",
        "# X_train - это массив, который содержит объекты из X_norm, выбранные случайным образом,\n",
        "# y_train - это массив, который содержит метки классов из y, соответствующие выбранным объектам,\n",
        "# а contamination - это доля объектов класса 1 в выборке.\n",
        "X_train, y_train, contamination = get_data_part(X_norm, y, size=0.2);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac24655",
      "metadata": {
        "id": "0ac24655"
      },
      "source": [
        "> Вывод train size: 56961\n",
        "contamination: 0.002 (0.2%)  Это значит, что мы обучаем модель на выборке из 56961 объектов, из которых 0.002 (0.2%) принадлежат классу 1, а остальные - классу 0. Класс 1 - это аномальные или необычные объекты, а класс 0 - это нормальные или типичные объекты. Мы хотим, чтобы модель могла обнаруживать аномалии в новых данных, которые не входят в выборку."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f208868",
      "metadata": {
        "id": "3f208868"
      },
      "source": [
        "Теоретическая справка\n",
        " Аномалии - это объекты, которые отличаются от большинства других объектов по своим признакам или поведению. Например, аномалией может быть мошенническая транзакция, неисправный датчик, редкий вид животного или необычный текст. Мы хотим, чтобы модель могла определить, является ли новый объект аномалией или нет, исходя из его сходства с обучающей выборкой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a47e055",
      "metadata": {
        "id": "2a47e055"
      },
      "outputs": [],
      "source": [
        "# Определяем функцию objective, которая принимает на вход объект trial, который представляет одно испытание оптимизации\n",
        "def objective(trial):\n",
        "    # Устанавливаем случайное зерно для воспроизводимости результатов\n",
        "    np.random.seed(42)\n",
        "    # Создаем объект модели kNN для обнаружения аномалий с помощью библиотеки pyod\n",
        "    clf = KNN(\n",
        "        # Задаем долю аномалий в обучающей выборке\n",
        "        contamination=contamination,\n",
        "        # Задаем количество соседей для kNN с помощью метода suggest_int, который выбирает случайное целое число\n",
        "        # из логарифмического распределения от 50 до 1000\n",
        "        n_neighbors=trial.suggest_int('n', 50, 1000, log=True),\n",
        "        # Задаем метод агрегации расстояний до соседей с помощью метода suggest_categorical,\n",
        "        # который выбирает случайное значение из списка ['largest', 'mean', 'median']\n",
        "        method=trial.suggest_categorical('method', ['largest', 'mean', 'median']),\n",
        "        # Задаем радиус для kNN с помощью метода suggest_float, который выбирает случайное вещественное число\n",
        "        # из равномерного распределения от 0.5 до 1\n",
        "        radius=trial.suggest_float('radius', 0.5, 1),\n",
        "        # Задаем метрику расстояния для kNN с помощью метода suggest_categorical, который выбирает случайное значение\n",
        "        # из списка ['l1', 'minkowski', 'l2']\n",
        "        metric=trial.suggest_categorical('metric', ['l1', 'minkowski', 'l2'])\n",
        "    )\n",
        "    # Обучаем модель на обучающей выборке X_train\n",
        "    clf.fit(X_train)\n",
        "    # Предсказываем метки классов для обучающей выборки X_train\n",
        "    y_pred = clf.labels_\n",
        "    # Возвращаем значение метрики качества обнаружения аномалий - среднюю точность (average precision score),\n",
        "    # которая вычисляется как площадь под кривой точность-полнота (precision-recall curve)\n",
        "    return average_precision_score(y_train, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a52c728",
      "metadata": {
        "id": "0a52c728"
      },
      "source": [
        "> Пояснение:\n",
        "metric=trial.suggest_categorical('metric', ['l1', 'minkowski', 'l2'])\n",
        "\n",
        "- `metric` - это имя параметра, который мы хотим оптимизировать. Он задает метрику расстояния для kNN, то есть способ измерения, насколько близки или далеки объекты друг от друга в пространстве признаков.\n",
        "- `trial` - это объект, который представляет одно испытание оптимизации. Он содержит методы, которые позволяют выбирать случайные значения для параметров из разных распределений или списков.\n",
        "- `suggest_categorical` - это метод, который принимает на вход имя параметра и список возможных значений для него. Он выбирает случайное значение из списка и возвращает его. Это значение будет использоваться для обучения модели в данном испытании.\n",
        "- `('metric', ['l1', 'minkowski', 'l2'])` - это аргументы, которые передаются в метод `suggest_categorical`. Они задают имя параметра `metric` и список возможных значений для него: `['l1', 'minkowski', 'l2']`. Это разные метрики расстояния, которые могут быть использованы для kNN. `l1` - это манхэттенское расстояние, `minkowski` - это обобщенное расстояние, которое включает в себя евклидово и манхэттенское расстояния в зависимости от параметра `p`, `l2` - это евклидово расстояние.\n",
        "Мы выбираем случайное значение из списка `['l1', 'minkowski', 'l2']` и присваивает его параметру `metric`. Это значение будет использоваться для вычисления расстояний между объектами в kNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed222b2d",
      "metadata": {
        "id": "ed222b2d"
      },
      "outputs": [],
      "source": [
        "# Вызываем функцию optimize, которая принимает на вход функцию objective, которая задает целевую функцию для оптимизации,\n",
        "# и параметр n, который задает количество испытаний для оптимизации\n",
        "# 20 - это количество испытаний, которые мы хотим провести для оптимизации параметров модели обнаружения аномалий.\n",
        "# Каждое испытание выбирает случайные значения для параметров модели из заданных диапазонов или списков с помощью\n",
        "# методов suggest_int, suggest_categorical и suggest_float. Затем оно обучает модель на обучающей выборке и вычисляет\n",
        "# значение целевой функции, которая равна средней точности (average precision score) на обучающей выборке.\n",
        "# Средняя точность - это метрика качества обнаружения аномалий, которая вычисляется как площадь под кривой точность-полнота\n",
        "# (precision-recall curve). Чем выше средняя точность, тем лучше модель обнаруживает аномалии.\n",
        "# После проведения всех испытаний мы можем увидеть лучшие параметры и лучшее значение целевой функции,\n",
        "# которые были найдены в процессе оптимизации, а также два графика, которые показывают важность параметров\n",
        "# и параллельные координаты. Важность параметров показывает, насколько каждый параметр влияет на значение целевой функции,\n",
        "# а параллельные координаты показывают, как параметры и значение целевой функции меняются в разных испытаниях.\n",
        "# Эти графики помогут нам понять, какие параметры лучше всего подходят для нашей модели.\n",
        "optimize(objective, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e42b8c8d",
      "metadata": {
        "id": "e42b8c8d"
      },
      "source": [
        "**Вывод**\n",
        "> Выводим на экран лучшие параметры и лучшее значение целевой функции, которые были найдены в процессе оптимизации с помощью функции optimize.\n",
        "Параметры меняются с обновлением:\n",
        "Best params:{'n': , 'method': 'median', 'radius': , 'metric': 'l1'}\n",
        "Best value:0.07\n",
        "Best params - это словарь с лучшими параметрами для модели kNN, которые были выбраны в ходе оптимизации. Они задают количество соседей (n), метод агрегации расстояний до соседей (method), радиус для kNN (radius) и метрику расстояния (metric) l2 - это евклидово расстояние.\n",
        "Best value - это лучшее значение целевой функции, которая вычисляет среднюю точность (average precision score) для модели kNN на обучающей выборке. Средняя точность - это метрика качества обнаружения аномалий, которая вычисляется как площадь под кривой точность-полнота (precision-recall curve). Чем выше средняя точность, тем лучше модель обнаруживает аномалии.\n",
        "0.07 - это числовое значение средней точности, которое было получено для модели kNN с лучшими параметрами. Оно показывает, что модель достаточно хорошо обнаруживает аномалии, но есть еще простор для улучшения."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d1d374",
      "metadata": {
        "id": "66d1d374"
      },
      "source": [
        "### LOF\n",
        "Это реализация метода обнаружения аномалий, который называется локальный коэффициент выброса (LOF). Этот метод вычисляет отклонение локальной плотности данной точки данных относительно ее соседей. Он считает выбросами те объекты, которые имеют значительно меньшую плотность, чем их соседи.\n",
        "\n",
        "PyOD LOF - это обертка класса scikit-learn LOF. PyOD - это библиотека Python для обнаружения аномалий, которая предоставляет различные алгоритмы и инструменты для этой задачи. Scikit-learn - это библиотека Python для машинного обучения, которая содержит множество моделей и методов для анализа данных. Класс LOF - это реализация метода LOF в scikit-learn, которая позволяет обучать модель и предсказывать аномалии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c9b4d2",
      "metadata": {
        "id": "a9c9b4d2"
      },
      "outputs": [],
      "source": [
        "# Импортируем библиотеку pyod, которая предоставляет различные алгоритмы для обнаружения аномалий в данных\n",
        "from pyod.models.lof import LOF\n",
        "# Импортируем класс LOF из модуля pyod.models.lof, который реализует метод локального коэффициента выброса (LOF)\n",
        "# для обнаружения аномалий\n",
        "# Метод LOF вычисляет отклонение локальной плотности данной точки данных относительно ее соседей.\n",
        "# Он считает выбросами те объекты, которые имеют значительно меньшую плотность, чем их соседи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e539ff39",
      "metadata": {
        "id": "e539ff39"
      },
      "outputs": [],
      "source": [
        "# Мы вызываем функцию get_data_part, которая принимает три аргумента:\n",
        "# X_norm, y и size.\n",
        "# X_norm - это массив, который содержит нормализованные значения признаков из исходных данных,\n",
        "# y - это массив, который содержит метки классов для каждого объекта из исходных данных,\n",
        "# а size - это размер выборки, которую мы хотим получить из исходных данных.\n",
        "# Функция get_data_part возвращает три значения:\n",
        "# X_train, y_train и contamination.\n",
        "# X_train - это массив, который содержит объекты из X_norm, выбранные случайным образом,\n",
        "# y_train - это массив, который содержит метки классов из y, соответствующие выбранным объектам,\n",
        "# а contamination - это доля объектов класса 1 в выборке.\n",
        "X_train, y_train, contamination = get_data_part(X_norm, y, size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74dc47e",
      "metadata": {
        "id": "c74dc47e"
      },
      "outputs": [],
      "source": [
        "# Определяем функцию objective, которая принимает на вход объект trial, который представляет одно испытание оптимизации\n",
        "def objective(trial):\n",
        "    # Устанавливаем случайное зерно для воспроизводимости результатов\n",
        "    np.random.seed(42)\n",
        "    # Создаем объект модели LOF для обнаружения аномалий с помощью библиотеки pyod\n",
        "    clf = LOF(\n",
        "        # Задаем долю аномалий в обучающей выборке\n",
        "        contamination=contamination,\n",
        "        # Задаем количество соседей для LOF с помощью метода suggest_int, который выбирает случайное целое число\n",
        "        # из логарифмического распределения от 50 до 1000\n",
        "        n_neighbors=trial.suggest_int('n_neighbors', 50, 1000, log=True),\n",
        "        # Задаем метрику расстояния для LOF с помощью метода suggest_categorical, который выбирает случайное значение\n",
        "        # из списка ['l2', 'l1', 'minkowski']\n",
        "        metric=trial.suggest_categorical('metric', ['l2', 'l1', 'minkowski'])\n",
        "    )\n",
        "    # Обучаем модель на обучающей выборке X_train\n",
        "    clf.fit(X_train)\n",
        "    # Предсказываем метки классов для обучающей выборки X_train\n",
        "    y_pred = clf.labels_\n",
        "    # Возвращаем значение метрики качества обнаружения аномалий - среднюю точность (average precision score),\n",
        "    # которая вычисляется как площадь под кривой точность-полнота (precision-recall curve)\n",
        "    return average_precision_score(y_train, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ae5048",
      "metadata": {
        "id": "13ae5048"
      },
      "outputs": [],
      "source": [
        "# Вызываем функцию optimize, которая принимает на вход функцию objective, которая задает целевую функцию для оптимизации,\n",
        "# и параметр n, который задает количество испытаний для оптимизации\n",
        "# 12 - это количество испытаний, которые мы хотим провести для оптимизации параметров модели обнаружения аномалий.\n",
        "# Каждое испытание выбирает случайные значения для параметров модели из заданных диапазонов или списков с помощью\n",
        "# методов suggest_int, suggest_categorical и suggest_float. Затем оно обучает модель на обучающей выборке и вычисляет\n",
        "# значение целевой функции, которая равна средней точности (average precision score) на обучающей выборке.\n",
        "# Средняя точность - это метрика качества обнаружения аномалий, которая вычисляется как площадь под кривой точность-полнота\n",
        "# (precision-recall curve). Чем выше средняя точность, тем лучше модель обнаруживает аномалии.\n",
        "# После проведения всех испытаний мы можем увидеть лучшие параметры и лучшее значение целевой функции,\n",
        "# которые были найдены в процессе оптимизации, а также два графика, которые показывают важность параметров\n",
        "# и параллельные координаты. Важность параметров показывает, насколько каждый параметр влияет на значение целевой функции,\n",
        "# а параллельные координаты показывают, как параметры и значение целевой функции меняются в разных испытаниях.\n",
        "# Эти графики помогут нам понять, какие параметры лучше всего подходят для нашей модели.\n",
        "optimize(objective, 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a7959b9",
      "metadata": {
        "id": "4a7959b9"
      },
      "source": [
        "**Вывод**\n",
        "Лучшие параметры для модели LOF, которые были найдены в процессе оптимизации, это:\n",
        "Параметры меняются с обновлением:\n",
        "- `n_neighbors`: - это количество соседей, которые используются для вычисления локальной плотности каждого объекта.\n",
        "- `metric`: 'minkowski' - это метрика расстояния, которая используется для измерения близости или дальности объектов в пространстве признаков. Метрика 'minkowski' - это обобщенная метрика, которая включает в себя евклидово и манхэттенское расстояния в зависимости от параметра `p`.\n",
        "-  лучшее значение целевой функции, которая вычисляет среднюю точность (average precision score) для модели LOF на обучающей выборке, равно 0.2.\n",
        "- Средняя точность - это метрика качества обнаружения аномалий, которая вычисляется как площадь под кривой точность-полнота (precision-recall curve). Чем выше средняя точность, тем лучше модель обнаруживает аномалии.\n",
        "- Значение 0.2 показывает, что модель достаточно хорошо обнаруживает аномалии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e65cbc",
      "metadata": {
        "id": "28e65cbc"
      },
      "outputs": [],
      "source": [
        "# Пояснения: этот код выполняться будет долго, пока закоментируем его\n",
        "#%%time\n",
        " ###### Используем магическую команду %%time, которая позволяет измерить время выполнения ячейки кода в Jupyter Notebook\n",
        "\n",
        " ###### Создаем объект модели LOF для обнаружения аномалий с помощью библиотеки pyod\n",
        "#clf = LOF(\n",
        "# Задаем долю аномалий в обучающей выборке\n",
        "   # contamination=contamination,\n",
        "# Задаем количество соседей для LOF, равное 280\n",
        "    # neighbors=280,###### Задаем метрику расстояния для LOF, равную l1, то есть манхэттенскому расстоянию\n",
        "    # metric='l1'\n",
        "# )\n",
        " ###### Обучаем модель на обучающей выборке X_train\n",
        "#clf.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79da60a3",
      "metadata": {
        "id": "79da60a3"
      },
      "outputs": [],
      "source": [
        "# metrics(clf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecea042b",
      "metadata": {
        "id": "ecea042b"
      },
      "source": [
        " > Вызоваем функцию metrics, которая оценивает качество модели обнаружения аномалий на обучающей выборке.\n",
        " - Функция metrics принимает на вход модель clf, которая была создана и обучена с помощью библиотеки pyod, и метки классов y_train, которые показывают, какие объекты являются аномалиями (класс 1) или нормальными (класс 0).\n",
        "- Функция metrics вычисляет и выводит на экран разные метрики, которые показывают, насколько хорошо модель clf согласуется с истинными метками классов. Эти метрики включают гомогенность, полноту, V-меру, точность, полноту, F-меру и поддержку для каждого класса. Функция metrics также строит и выводит на экран матрицу ошибок, которая показывает, сколько объектов каждого класса было правильно или неправильно отнесено к другому классу."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa39250",
      "metadata": {
        "id": "8fa39250"
      },
      "source": [
        "> Теоретическая справка\n",
        "Кластеризация - это задача разбиения данных на группы (кластеры) так, чтобы объекты внутри одного кластера были похожи друг на друга, а объекты из разных кластеров - различны. Метки классов - это истинные принадлежности объектов к кластерам, которые могут быть известны или неизвестны. Модель кластеризации - это алгоритм, который пытается найти оптимальное разбиение данных на кластеры. Модель может предсказать метки кластеров для новых объектов, которые не входят в обучающую выборку.\n",
        "\n",
        " > Метрики:\n",
        " - Гомогенность - это мера того, насколько каждый кластер содержит только объекты одного класса. Гомогенность равна 1, если все кластеры гомогенны, и меньше 1, если в кластерах есть объекты разных классов.\n",
        "- Полнота - это мера того, насколько все объекты одного класса попадают в один кластер. Полнота равна 1, если все объекты одного класса собраны в один кластер, и меньше 1, если объекты одного класса разбросаны по разным кластерам.\n",
        "- V-мера - это среднее гармоническое между гомогенностью и полнотой. V-мера равна 1, если и гомогенность, и полнота равны 1, и меньше 1, если одна из них меньше 1. V-мера показывает, насколько хорошо модель кластеризации соответствует истинному разбиению данных на классы.\n",
        "- Точность - это доля правильно предсказанных меток классов среди всех предсказанных меток. Точность равна 1, если все метки предсказаны верно, и меньше 1, если есть ошибки в предсказаниях. Точность показывает, насколько точно модель кластеризации определяет принадлежность объектов к классам.\n",
        "- Полнота - это доля правильно предсказанных меток классов среди всех истинных меток. Полнота равна 1, если все истинные метки предсказаны верно, и меньше 1, если есть пропуски в предсказаниях. Полнота показывает, насколько полно модель кластеризации учитывает все объекты каждого класса.\n",
        "- F-мера - это среднее гармоническое между точностью и полнотой. F-мера равна 1, если и точность, и полнота равны 1, и меньше 1, если одна из них меньше 1. F-мера показывает, насколько хорошо модель кластеризации сбалансирована между точностью и полнотой.\n",
        "- Поддержка - это количество объектов каждого класса в обучающей выборке. Поддержка показывает, насколько представительны данные для каждого класса."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f486f4a",
      "metadata": {
        "id": "9f486f4a"
      },
      "source": [
        "### Isolation Forest\n",
        "\n",
        "- Isolation Forest - это алгоритм для обнаружения аномалий в данных, который использует бинарные деревья для изоляции аномальных точек на основе их длины пути. Он имеет линейную временную сложность, низкое требование к памяти и хорошо работает с высокоразмерными данными.\n",
        "- Isolation Forest работает так: для каждого объекта он находит k ближайших соседей в обучающей выборке, то есть те объекты, которые имеют наименьшее расстояние до данного объекта. Затем он вычисляет аномальный коэффициент для каждого объекта как среднее расстояние до его соседей. Чем больше аномальный коэффициент, тем больше вероятность, что объект является аномалией."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyod"
      ],
      "metadata": {
        "id": "Fy2K0OoVUjXV"
      },
      "id": "Fy2K0OoVUjXV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bad3b01",
      "metadata": {
        "id": "1bad3b01"
      },
      "outputs": [],
      "source": [
        "# импортируем модуль pyod.models.iforest, который содержит класс IForest для обнаружения аномалий с помощью изолирующего леса\n",
        "# изолирующий лес - это алгоритм машинного обучения, который строит множество случайных деревьев и измеряет, насколько легко изолировать каждую точку данных\n",
        "# чем больше аномальна точка данных, тем меньше разделений ей нужно, чтобы быть изолированной от остальных\n",
        "from pyod.models.iforest import IForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f736a7",
      "metadata": {
        "id": "22f736a7"
      },
      "outputs": [],
      "source": [
        "# задаем нормализованные данные X_norm и метки y, которые хотим разделить\n",
        "\n",
        "# вызываем функцию get_data_part с аргументами X_norm, y и size\n",
        "# X_norm - это двумерный массив с данными\n",
        "# y - это одномерный массив с метками, где 0 - нормальное значение, а 1 - аномальное\n",
        "# size - это доля данных, которая будет использована для обучающей выборки\n",
        "# функция возвращает три значения: X_train, y_train и contamination\n",
        "# X_train - это двумерный массив с обучающими данными\n",
        "# y_train - это одномерный массив с обучающими метками\n",
        "# contamination - это доля аномалий в обучающей выборке\n",
        "\n",
        "X_train, y_train, contamination = get_data_part(X_norm, y, size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd96ba82",
      "metadata": {
        "id": "fd96ba82"
      },
      "source": [
        "- Размер выборки 85442\n",
        "- доля аномаоий 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c770c16",
      "metadata": {
        "id": "7c770c16"
      },
      "outputs": [],
      "source": [
        "# определяем функцию objective, которая принимает аргумент trial\n",
        "def objective(trial):\n",
        "    # создаем объект класса IForest, который использует алгоритм изолирующего леса для обнаружения аномалий\n",
        "    # contamination - это доля аномалий в данных, которая задается заранее\n",
        "    # random_state - это параметр для воспроизводимости результатов\n",
        "    # n_estimators - это количество деревьев в изолирующем лесе, которое подбирается с помощью метода suggest_int\n",
        "    # метод suggest_int принимает имя параметра, нижнюю и верхнюю границы и шаг для выбора целочисленного значения\n",
        "    # параметр log=True означает, что значения выбираются из логарифмического пространства\n",
        "    clf = IForest(\n",
        "        contamination=contamination,\n",
        "        random_state=0,\n",
        "        n_estimators=trial.suggest_int('n', 50, 1000, log=True)\n",
        "    )\n",
        "    # обучаем модель на обучающих данных X_train\n",
        "    clf.fit(X_train)\n",
        "    # получаем предсказанные метки для обучающих данных\n",
        "    y_pred = clf.labels_\n",
        "    # возвращаем значение метрики average_precision_score, которая вычисляет среднюю точность по кривой точность-полнота\n",
        "    # метрика принимает истинные метки y_train и предсказанные метки y_pred\n",
        "    return average_precision_score(y_train, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950c9a36",
      "metadata": {
        "id": "950c9a36"
      },
      "outputs": [],
      "source": [
        "# Вызываем функцию optimize, которая принимает на вход функцию objective, которая задает целевую функцию для оптимизации,\n",
        "# и параметр n, который задает количество испытаний для оптимизации\n",
        "# 10 - это количество испытаний, которые мы хотим провести для оптимизации параметров модели обнаружения аномалий.\n",
        "# Каждое испытание выбирает случайные значения для параметров модели из заданных диапазонов или списков с помощью\n",
        "# методов suggest_int, suggest_categorical и suggest_float. Затем оно обучает модель на обучающей выборке и вычисляет\n",
        "# значение целевой функции, которая равна средней точности (average precision score) на обучающей выборке.\n",
        "# Средняя точность - это метрика качества обнаружения аномалий, которая вычисляется как площадь под кривой точность-полнота\n",
        "# (precision-recall curve). Чем выше средняя точность, тем лучше модель обнаруживает аномалии.\n",
        "# После проведения всех испытаний мы можем увидеть лучшие параметры и лучшее значение целевой функции,\n",
        "# которые были найдены в процессе оптимизации, а также два графика, которые показывают важность параметров\n",
        "# и параллельные координаты. Важность параметров показывает, насколько каждый параметр влияет на значение целевой функции,\n",
        "# а параллельные координаты показывают, как параметры и значение целевой функции меняются в разных испытаниях.\n",
        "# Эти графики помогут нам понять, какие параметры лучше всего подходят для нашей модели.\n",
        "optimize(objective, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7ac70f",
      "metadata": {
        "id": "2b7ac70f"
      },
      "source": [
        "**Вывод**\n",
        "- Наша модель достигла наивысшего значения метрики average_precision_score, которая измеряет качество обнаружения аномалий.\n",
        "Параметры меняются с обновлением:\n",
        "Лучшие параметры - это {'n': }, что означает, что изолирующий лес состоит из n деревьев.\n",
        "- Лучшее значение - это 0.09, что означает, что средняя точность по кривой точность-полнота составляет примерно n %. Это довольно низкое значение, что может свидетельствовать о том, что  данные сложны для обнаружения аномалий или что нужно попробовать другой метод или другие параметры."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe75218",
      "metadata": {
        "id": "bfe75218"
      },
      "source": [
        "### HBOS\n",
        "\n",
        "- HBOS - это сокращение от Histogram-based Outlier Score, что означает оценка аномалий на основе гистограмм.\n",
        "- Это эффективный метод обнаружения аномалий без учителя. Он предполагает, что признаки независимы и вычисляет степень выброса, построив гистограммы для каждого признака. Этот методреализован в библиотеке PyOD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ca51d24",
      "metadata": {
        "id": "7ca51d24"
      },
      "outputs": [],
      "source": [
        "# импортируем модуль pyod.models.hbos, который содержит класс HBOS для обнаружения аномалий с помощью гистограмм\n",
        "# HBOS - это сокращение от Histogram-based Outlier Score, что означает оценка аномалий на основе гистограмм\n",
        "# Это эффективный метод обнаружения аномалий без учителя, который работает быстрее, чем многомерные методы, но менее точно\n",
        "\n",
        "from pyod.models.hbos import HBOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035d670d",
      "metadata": {
        "id": "035d670d"
      },
      "outputs": [],
      "source": [
        "# задаем нормализованные данные X_norm и метки y, которые хотим разделить\n",
        "\n",
        "# вызываем функцию get_data_part с аргументами X_norm, y и size\n",
        "# X_norm - это двумерный массив с данными\n",
        "# y - это одномерный массив с метками, где 0 - нормальное значение, а 1 - аномальное\n",
        "# size - это доля данных, которая будет использована для обучающей выборки\n",
        "# функция возвращает три значения: X_train, y_train и contamination\n",
        "# X_train - это двумерный массив с обучающими данными\n",
        "# y_train - это одномерный массив с обучающими метками\n",
        "# contamination - это доля аномалий в обучающей выборке\n",
        "X_train, y_train, contamination = get_data_part(X_norm, y, size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5679d08a",
      "metadata": {
        "id": "5679d08a"
      },
      "source": [
        "- Размер выборки 85442\n",
        "- доля аномаоий 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d627e4",
      "metadata": {
        "id": "a0d627e4"
      },
      "outputs": [],
      "source": [
        "# определяем функцию objective, которая принимает аргумент trial\n",
        "def objective(trial):\n",
        "    # создаем объект класса HBOS, который использует алгоритм обнаружения аномалий на основе гистограмм\n",
        "    # contamination - это доля аномалий в данных, которая задается заранее\n",
        "    # n_bins - это количество бинов для построения гистограмм, 'auto' означает, что оно будет определено автоматически\n",
        "    # alpha - это параметр, который контролирует ширину бинов, чем больше alpha, тем меньше бинов\n",
        "    # tol - это параметр, который контролирует порог для определения аномалий, чем больше tol, тем больше аномалий\n",
        "    # alpha и tol подбираются с помощью метода suggest_float, который возвращает случайное вещественное число\n",
        "    # из заданного интервала\n",
        "    # параметр log=True означает, что значения выбираются из логарифмического пространства\n",
        "    clf = HBOS(\n",
        "        contamination=contamination,\n",
        "        n_bins='auto',\n",
        "        alpha=trial.suggest_float('alpha', 0.05, 0.2),\n",
        "        tol=trial.suggest_float('tol', 0.05, 0.9, log=True)\n",
        "    )\n",
        "    # обучаем модель на обучающих данных X_train\n",
        "    clf.fit(X_train)\n",
        "    # получаем предсказанные метки для обучающих данных\n",
        "    y_pred = clf.labels_\n",
        "    # возвращаем значение метрики average_precision_score, которая вычисляет среднюю точность по кривой точность-полнота\n",
        "    # метрика принимает истинные метки y_train и предсказанные метки y_pred\n",
        "\n",
        "    return average_precision_score(y_train, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa517a49",
      "metadata": {
        "id": "aa517a49"
      },
      "outputs": [],
      "source": [
        "# Вызываем функцию optimize, которая принимает на вход функцию objective, которая задает целевую функцию для оптимизации,\n",
        "# и параметр n, который задает количество испытаний для оптимизации\n",
        "# 12 - это количество испытаний, которые мы хотим провести для оптимизации параметров модели обнаружения аномалий.\n",
        "# Каждое испытание выбирает случайные значения для параметров модели из заданных диапазонов или списков с помощью\n",
        "# методов suggest_int, suggest_categorical и suggest_float. Затем оно обучает модель на обучающей выборке и вычисляет\n",
        "# значение целевой функции, которая равна средней точности (average precision score) на обучающей выборке.\n",
        "# Средняя точность - это метрика качества обнаружения аномалий, которая вычисляется как площадь под кривой точность-полнота\n",
        "# (precision-recall curve). Чем выше средняя точность, тем лучше модель обнаруживает аномалии.\n",
        "# После проведения всех испытаний мы можем увидеть лучшие параметры и лучшее значение целевой функции,\n",
        "# которые были найдены в процессе оптимизации, а также два графика, которые показывают важность параметров\n",
        "# и параллельные координаты. Важность параметров показывает, насколько каждый параметр влияет на значение целевой функции,\n",
        "# а параллельные координаты показывают, как параметры и значение целевой функции меняются в разных испытаниях.\n",
        "# Эти графики помогут нам понять, какие параметры лучше всего подходят для нашей модели.\n",
        "\n",
        "optimize(objective, 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb5c812",
      "metadata": {
        "id": "2eb5c812"
      },
      "source": [
        "**Вывод **\n",
        "Параметры меняются с обновлением:\n",
        "- Лучшие параметры для модели HBOS.\n",
        "наша модель достигла наивысшего значения метрики average_precision_score, которая измеряет качество обнаружения аномалий. - - Лучшие параметры - это {'alpha': 0.051, 'tol': 0.50}, что означает, что гистограмма имеет ширину бинов 0.0511 и порог для определения аномалий 0.50.\n",
        "- Лучшее значение - это 0.107, что означает, что средняя точность по кривой точность-полнота составляет примерно 10.7%.\n",
        "Это немного лучше, чем с моделью IForest, но все еще довольно низкое значение, что может свидетельствовать о том, что данные сложны для обнаружения аномалий или что нужно попробовать другой метод или другие параметры."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f6609c",
      "metadata": {
        "id": "55f6609c"
      },
      "source": [
        "### Parameter-free models\n",
        "\n",
        "Parameter-free models - это модели, которые не требуют никаких параметров, заданных пользователем, для выполнения обнаружения аномалий. Они также называются непараметрическими моделями, потому что они не предполагают никакого конкретного распределения для данных. Parameter-free models полезны, когда данные сложны, высокоразмерны или имеют неизвестные характеристики.\n",
        "- Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions (ECOD): Эта модель использует эмпирическую кумулятивную функцию распределения (ECDF) каждого признака для измерения выброса каждой точки данных. ECDF - это доля точек данных, которые меньше или равны заданному значению. Модель вычисляет ECDF-оценку для каждой точки данных как произведение ECDF-значений всех признаков, а затем ранжирует точки данных по их ECDF-оценкам. Чем ниже ECDF-оценка, тем больше вероятность, что точка данных является аномальной.\n",
        "- Rotation-based Outlier Detector (ROD): Эта модель использует метод, основанный на повороте, для преобразования данных в пространство меньшей размерности, где выбросы более заметны. Модель поворачивает данные так, чтобы максимизировать разницу между средними значениями нормальных и аномальных точек в каждом направлении. Затем модель вычисляет ROD-оценку для каждой точки данных как сумму квадратов расстояний от средних значений в каждом направлении. Чем выше ROD-оценка, тем больше вероятность, что точка данных является аномальной.\n",
        "- Copula Based Outlier Detector (COPOD): Эта модель использует копулы для моделирования зависимости между признаками данных. Копула - это функция, которая связывает маргинальные распределения признаков в совместное распределение. Модель вычисляет COPOD-оценку для каждой точки данных как произведение верхней и нижней копулы, которые измеряют вероятность того, что точка данных превышает или меньше остальных точек по всем признакам. Чем выше COPOD-оценка, тем больше вероятность, что точка данных является аномальной."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7905cb3e",
      "metadata": {
        "id": "7905cb3e"
      },
      "outputs": [],
      "source": [
        "from pyod.models.ecod import ECOD\n",
        "from pyod.models.copod import COPOD\n",
        "from pyod.models.rod import ROD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf147c1",
      "metadata": {
        "id": "fdf147c1"
      },
      "outputs": [],
      "source": [
        "# импортируем модуль pyod.models.ecod, который содержит класс ECOD для обнаружения аномалий с помощью эмпирических кумулятивных\n",
        "# функций распределения\n",
        "# ECOD - это сокращение от Empirical-Cumulative-distribution-based Outlier Detection, что означает оценка аномалий на\n",
        "# основе эмпирических кумулятивных функций распределения\n",
        "# Это эффективный метод обнаружения аномалий без учителя, который работает быстрее, чем многомерные методы, но менее точно.\n",
        "\n",
        "from pyod.models.ecod import ECOD\n",
        "\n",
        "# импортируем модуль pyod.models.copod, который содержит класс COPOD для обнаружения аномалий с помощью копул\n",
        "# COPOD - это сокращение от Copula-Based Outlier Detection, что означает оценка аномалий на основе копул\n",
        "# Это новый метод обнаружения аномалий без учителя, который вдохновлен статистическими методами для моделирования\n",
        "# многомерного распределения данных\n",
        "\n",
        "from pyod.models.copod import COPOD\n",
        "\n",
        "# импортируем модуль pyod.models.rod, который содержит класс ROD для обнаружения аномалий с помощью метода, основанного\n",
        "# на повороте\n",
        "# ROD - это сокращение от Rotation-based Outlier Detector, что означает детектор аномалий, основанный на повороте\n",
        "# Это еще один новый метод обнаружения аномалий без учителя, который использует технику поворота для преобразования\n",
        "# данных в пространство меньшей размерности, где выбросы более заметны\n",
        "\n",
        "from pyod.models.rod import ROD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eff4428",
      "metadata": {
        "id": "4eff4428"
      },
      "outputs": [],
      "source": [
        "# задаем нормализованные данные X_norm и метки y, которые хотим разделить\n",
        "\n",
        "# вызываем функцию get_data_part с аргументами X_norm, y и size\n",
        "# X_norm - это двумерный массив с данными\n",
        "# y - это одномерный массив с метками, где 0 - нормальное значение, а 1 - аномальное\n",
        "# size - это доля данных, которая будет использована для обучающей выборки\n",
        "# функция возвращает три значения: X_train, y_train и contamination\n",
        "# X_train - это двумерный массив с обучающими данными\n",
        "# y_train - это одномерный массив с обучающими метками\n",
        "# contamination - это доля аномалий в обучающей выборке\n",
        "X_train, y_train, contamination = get_data_part(X_norm, y, size=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f007e46",
      "metadata": {
        "id": "7f007e46"
      },
      "source": [
        "- Размер выборки 227845\n",
        "- доля аномаоий 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f54dac8",
      "metadata": {
        "id": "9f54dac8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Используем магическую команду %%time для измерения времени выполнения ячейки\n",
        "\n",
        "# Создаем объект класса ECOD, который использует алгоритм обнаружения аномалий на основе эмпирических кумулятивных функций\n",
        "# распределения\n",
        "# contamination - это доля аномалий в данных, которая задается заранее\n",
        "ecod = ECOD(contamination=contamination)\n",
        "# Обучаем модель на обучающих данных X_train\n",
        "ecod.fit(X_train)\n",
        "# Вызываем функцию metrics, которая вычисляет и выводит различные метрики качества модели, такие как точность,\n",
        "# полнота, F1-мера и AUC\n",
        "# Функция принимает два аргумента: модель и истинные метки y_train\n",
        "metrics(ecod, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1760ae9",
      "metadata": {
        "id": "b1760ae9"
      },
      "source": [
        "**Вывод** , ниже описано как модель ECOD справилась с обнаружением аномалий на обучающих данных.\n",
        "- ROC: это сокращение от Receiver Operating Characteristic, что означает характеристика работы приемника. Это кривая, которая показывает, как меняется соотношение между долей верно классифицированных аномалий (True Positive Rate) и долей неверно классифицированных нормальных точек (False Positive Rate) при изменении порога для определения аномалий. Чем выше ROC, тем лучше модель разделяет аномалии и нормальные точки. К сожалению, наша модель имеет ROC 0.6668, что означает, что она не очень хорошо справляется с этой задачей.\n",
        "- Precision: это доля точек данных, которые модель правильно определила как аномальные, среди всех точек, которые модель определила как аномальные. Чем выше precision, тем меньше ложных срабатываний модели. Модель имеет precision 0.29, что означает, что из всех точек, которые модель пометила как аномальные, только 29% действительно являются аномальными.\n",
        "- Recall: это доля точек данных, которые модель правильно определила как аномальные, среди всех аномальных точек в данных. Чем выше recall, тем меньше пропущенных аномалий моделью. Модель имеет recall 0.34, что означает, что из всех аномальных точек в данных, модель нашла только 34%.\n",
        "- F1-score: это среднее гармоническое между precision и recall, которое учитывает их баланс. Чем выше F1-score, тем лучше модель согласовывает precision и recall. Модель имеет F1-score 0.31, что означает, что она не очень хорошо балансирует между precision и recall.\n",
        "- Confusion matrix: это таблица, которая показывает, сколько точек данных было правильно или неправильно классифицировано моделью. Модель имеет confusion matrix [[227129    325]\n",
        " [   260    131]], что означает, что она правильно определила 227129 нормальных точек и 131 аномальную точку, но также ошибочно определила 325 нормальных точек как аномальные и пропустила 260 аномальных точек."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9549e548",
      "metadata": {
        "id": "9549e548"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Используем магическую команду %%time для измерения времени выполнения ячейки\n",
        "\n",
        "# Создаем объект класса COPOD, который использует алгоритм обнаружения аномалий на основе копул\n",
        "# contamination - это доля аномалий в данных, которая задается заранее\n",
        "copod = COPOD(contamination=contamination)\n",
        "# Обучаем модель на обучающих данных X_train\n",
        "copod.fit(X_train)\n",
        "# Вызываем функцию metrics, которая вычисляет и выводит различные метрики качества модели, такие как точность, полнота, F1-мера и AUC\n",
        "# Функция принимает два аргумента: модель и истинные метки y_train\n",
        "metrics(copod, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbc75a8d",
      "metadata": {
        "id": "cbc75a8d"
      },
      "source": [
        "**Вывод** посмотрим как модель COPOD справилась с обнаружением аномалий на обучающих данных.\n",
        "\n",
        "- ROC: Это кривая, которая показывает, как меняется соотношение между долей верно классифицированных аномалий (True Positive Rate) и долей неверно классифицированных нормальных точек (False Positive Rate) при изменении порога для определения аномалий. Чем выше ROC, тем лучше модель разделяет аномалии и нормальные точки. Модель имеет ROC 0.6988, что означает, что она лучше справляется с этой задачей, чем модель ECOD.\n",
        "- Precision: это доля точек данных, которые модель правильно определила как аномальные, среди всех точек, которые модель определила как аномальные. Чем выше precision, тем меньше ложных срабатываний модели. Модель имеет precision 0.34, что означает, что из всех точек, которые модель пометила как аномальные, 34% действительно являются аномальными. Это немного лучше, чем у модели ECOD.\n",
        "- Recall: это доля точек данных, которые модель правильно определила как аномальные, среди всех аномальных точек в данных. Чем выше recall, тем меньше пропущенных аномалий моделью. Модель имеет recall 0.40, что означает, что из всех аномальных точек в данных, модель нашла 40%. Это намного лучше, чем у модели ECOD.\n",
        "- F1-score: это среднее гармоническое между precision и recall, которое учитывает их баланс. Чем выше F1-score, тем лучше модель согласовывает precision и recall. Модель имеет F1-score 0.37, что означает, что она лучше балансирует между precision и recall, чем модель ECOD.\n",
        "- Confusion matrix: это таблица, которая показывает, сколько точек данных было правильно или неправильно классифицировано моделью. Ваша модель имеет confusion matrix [[227154    300]\n",
        " [   235    156]], что означает, что она правильно определила 227154 нормальных точек и 156 аномальных точек, но также ошибочно определила 300 нормальных точек как аномальные и пропустила 235 аномальных точек. Это лучше, чем у модели ECOD.\n",
        "\n",
        "- Из этих метрик вы можете сделать вывод, что модель COPOD лучше справляется с обнаружением аномалий на обучающих данных, чем модель ECOD. Возможно, мы должны попробовать эту модель на тестовых данных или сравнить ее с другими моделями."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68bd27ab",
      "metadata": {
        "id": "68bd27ab"
      },
      "source": [
        "### ROD\n",
        "\n",
        "ROD - это сокращение от Rotation-based Outlier Detection, что означает детектор аномалий, основанный на повороте. Это устойчивый и безпараметрический алгоритм, который не требует никаких предположений о статистическом распределении данных и работает интуитивно в трехмерном пространстве, где 3D-векторы, представляющие точки данных, поворачиваются вокруг геометрической медианы два раза против часовой стрелки с помощью формулы поворота Родрига. Результаты поворота - это параллелепипеды, чьи объемы математически анализируются как целевые функции и используются для вычисления медианного абсолютного отклонения, чтобы получить оценку выброса. Для высоких размерностей > 3 общая оценка вычисляется путем усреднения общих оценок 3D-подпространств, которые были получены в результате разложения исходного пространства данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9181c79b",
      "metadata": {
        "id": "9181c79b"
      },
      "outputs": [],
      "source": [
        "# задаем нормализованные данные X_norm и метки y, которые хотим разделить\n",
        "\n",
        "# вызываем функцию get_data_part с аргументами X_norm, y и size\n",
        "# X_norm - это двумерный массив с данными\n",
        "# y - это одномерный массив с метками, где 0 - нормальное значение, а 1 - аномальное\n",
        "# size - это доля данных, которая будет использована для обучающей выборки\n",
        "# функция возвращает три значения: X_train, y_train и contamination\n",
        "# X_train - это двумерный массив с обучающими данными\n",
        "# y_train - это одномерный массив с обучающими метками\n",
        "# contamination - это доля аномалий в обучающей выборке\n",
        "X_train, y_train, contamination = get_data_part(X_norm, y, size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02b1a83",
      "metadata": {
        "id": "c02b1a83"
      },
      "source": [
        "- Размер выборки 85442\n",
        "- доля аномаоий 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87988e93",
      "metadata": {
        "id": "87988e93"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# rod = ROD(contamination=contamination)\n",
        "# rod.fit(X_train)\n",
        "# metrics(rod, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfddefe",
      "metadata": {
        "id": "fcfddefe"
      },
      "source": [
        "**Внимание: ниже код работает очень долго.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e43c178",
      "metadata": {
        "id": "5e43c178"
      },
      "outputs": [],
      "source": [
        "#%%time\n",
        "# Используем магическую команду %%time для измерения времени выполнения ячейки\n",
        "\n",
        "# Создаем объект класса ROD, который использует алгоритм обнаружения аномалий на основе метода, основанного на повороте\n",
        "# contamination - это доля аномалий в данных, которая задается заранее\n",
        "#rod = ROD(contamination=contamination)\n",
        "# Обучаем модель на обучающих данных X_train\n",
        "#rod.fit(X_train)\n",
        "# Вызываем функцию metrics, которая вычисляет и выводит различные метрики качества модели, такие как точность, полнота, F1-мера и AUC\n",
        "# Функция принимает два аргумента: модель и истинные метки y_train\n",
        "#metrics(rod, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df646c8d",
      "metadata": {
        "id": "df646c8d"
      },
      "source": [
        "ROD работает слишком долго даже на 30% данных..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6963281",
      "metadata": {
        "id": "f6963281"
      },
      "source": [
        "### Deep SVD\n",
        "\n",
        "Deep SVD - это сокращение от Deep Singular Value Decomposition, что означает глубокое сингулярное разложение. Это метод для построения нейронной сети, которая может аппроксимировать матрицу рейтингов пользователей и объектов с помощью трех матриц: U, Σ и V^T. Этот метод основан на классическом алгоритме сингулярного разложения (SVD), который разбивает матрицу на три матрицы, такие что A = U Σ V^T, где U и V - это ортогональные матрицы, а Σ - это диагональная матрица с сингулярными значениями. Deep SVD использует нейронную сеть для обучения матриц U и V, а также для оптимизации сингулярных значений в Σ. Это позволяет учитывать нелинейные зависимости между пользователями и объектами, а также уменьшать размерность пространства признаков. Deep SVD может быть использован для решения задачи рекомендательных систем, то есть предсказания рейтингов пользователей для объектов, которые они еще не оценили.\n",
        "![%D0%A1%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D0%B5%20%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.jpeg](attachment:%D0%A1%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D0%B5%20%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.jpeg)![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f43d584d",
      "metadata": {
        "id": "f43d584d"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow\n",
        "# обычная установка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f0ac860",
      "metadata": {
        "id": "7f0ac860"
      },
      "outputs": [],
      "source": [
        "# pip install --upgrade tensorflow # или обновление, если нужно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7ae9838",
      "metadata": {
        "id": "a7ae9838"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd989a8",
      "metadata": {
        "id": "2cd989a8"
      },
      "outputs": [],
      "source": [
        "# импортируем модуль pyod.models.deep_svdd, который содержит класс DeepSVDD для обнаружения аномалий с помощью\n",
        "# глубокого одноклассового метода\n",
        "# DeepSVDD - это сокращение от Deep Support Vector Data Description, что означает глубокое описание данных\n",
        "# с помощью опорных векторов\n",
        "# Это метод для обучения нейронной сети, которая отображает данные в пространство, где они ограничены гиперсферой\n",
        "# с центром c и радиусом R\n",
        "# Метод минимизирует объем гиперсферы, заставляя сеть извлекать общие факторы вариации в данных\n",
        "# Метод может быть использован для обнаружения аномалий, вычисляя расстояние от центра гиперсферы до каждой точки данных\n",
        "# Чем больше расстояние, тем больше вероятность, что точка данных является аномальной\n",
        "\n",
        "from pyod.models.deep_svdd import DeepSVDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b52980",
      "metadata": {
        "id": "66b52980"
      },
      "outputs": [],
      "source": [
        "# задаем нормализованные данные X_norm и метки y, которые хотим разделить\n",
        "\n",
        "# вызываем функцию get_data_part с аргументами X_norm, y и size\n",
        "# X_norm - это двумерный массив с данными\n",
        "# y - это одномерный массив с метками, где 0 - нормальное значение, а 1 - аномальное\n",
        "# size - это доля данных, которая будет использована для обучающей выборки\n",
        "# функция возвращает три значения: X_train, y_train и contamination\n",
        "# X_train - это двумерный массив с обучающими данными\n",
        "# y_train - это одномерный массив с обучающими метками\n",
        "# contamination - это доля аномалий в обучающей выборке\n",
        "\n",
        "X_train, y_train, contamination = get_data_part(X_norm, y, size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d106ebd",
      "metadata": {
        "id": "6d106ebd"
      },
      "source": [
        "- Размер выборки 28480\n",
        "- доля аномаоий 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76bd65f7",
      "metadata": {
        "id": "76bd65f7"
      },
      "outputs": [],
      "source": [
        "# определяем функцию цели для оптимизации гиперпараметров с помощью optuna\n",
        "def objective(trial):\n",
        "    # получаем количество нейронов во входном слое по размеру обучающих данных\n",
        "    neurons = X_train.shape[1]\n",
        "    # создаем объект класса DeepSVDD, который использует алгоритм обнаружения аномалий на основе глубокого одноклассового метода\n",
        "    # random_state - это параметр для воспроизводимости результатов\n",
        "    # contamination - это доля аномалий в данных, которая задается заранее\n",
        "    # verbose - это параметр для управления выводом информации во время обучения\n",
        "    # hidden_neurons - это список, содержащий количество нейронов в скрытых слоях сети\n",
        "    # use_ae - это булевый параметр, который определяет, использовать ли автоэнкодер для инициализации весов сети\n",
        "    # epochs - это количество эпох для обучения сети\n",
        "    # dropout_rate - это параметр для регуляризации сети, который определяет долю нейронов, которые отключаются во время обучения\n",
        "    # l2_regularizer - это параметр для регуляризации сети, который определяет коэффициент L2-штрафа для весов сети\n",
        "    # все эти параметры, кроме random_state и contamination, являются гиперпараметрами, которые мы хотим оптимизировать с помощью optuna\n",
        "    # для этого мы используем различные методы trial.suggest_*, которые позволяют выбирать значения из заданного диапазона или списка\n",
        "    # например, trial.suggest_categorical выбирает значение из списка категориальных значений\n",
        "    # trial.suggest_int выбирает целочисленное значение из заданного диапазона с заданным шагом\n",
        "    # trial.suggest_float выбирает вещественное значение из заданного диапазона с возможностью использовать логарифмическую шкалу\n",
        "    clf = DeepSVDD(\n",
        "        random_state=0,\n",
        "        contamination=contamination,\n",
        "        verbose=0,\n",
        "        hidden_neurons=[neurons, round(neurons / 2)],\n",
        "        use_ae=trial.suggest_categorical('use_ae', [True, False]),\n",
        "        epochs=trial.suggest_int('epochs', 100, 300),\n",
        "        dropout_rate=trial.suggest_float('dropout', 0.05, 0.5, log=True),\n",
        "        l2_regularizer=trial.suggest_float('l2', 0.05, 0.5, log=True)\n",
        "    )\n",
        "    # обучаем модель на обучающих данных X_train\n",
        "    clf.fit(X_train)\n",
        "    # получаем метки аномалий для тестовых данных X_test\n",
        "    y_pred = clf.labels_\n",
        "    # возвращаем среднюю точность (average precision score) как метрику качества модели\n",
        "    # средняя точность - это взвешенное среднее точностей, достигнутых при различных порогах, с увеличением полноты от предыдущего порога, используемого в качестве веса\n",
        "    # средняя точность лежит в диапазоне от 0 до 1, чем выше, тем лучше\n",
        "    return average_precision_score(y_train, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a6f7af",
      "metadata": {
        "id": "a8a6f7af"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    neurons = X_train.shape[1]\n",
        "    clf = DeepSVDD(\n",
        "        random_state=0,\n",
        "        contamination=contamination,\n",
        "        verbose=0,\n",
        "        hidden_neurons=[neurons, round(neurons / 2)],\n",
        "        use_ae=trial.suggest_categorical('use_ae', [True, False]),\n",
        "        epochs=trial.suggest_int('epochs', 100, 300),\n",
        "        dropout_rate=trial.suggest_float('dropout', 0.05, 0.5, log=True),\n",
        "        l2_regularizer=trial.suggest_float('l2', 0.05, 0.5, log=True)\n",
        "    )\n",
        "    clf.fit(X_train)\n",
        "    y_pred = clf.labels_\n",
        "    return average_precision_score(y_train, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb517cc",
      "metadata": {
        "id": "bdb517cc"
      },
      "outputs": [],
      "source": [
        "# Вызываем функцию optimize, которая принимает на вход функцию objective, которая задает целевую функцию для оптимизации,\n",
        "# и параметр n, который задает количество испытаний для оптимизации\n",
        "# 20 - это количество испытаний, которые мы хотим провести для оптимизации параметров модели обнаружения аномалий.\n",
        "# Каждое испытание выбирает случайные значения для параметров модели из заданных диапазонов или списков с помощью\n",
        "# методов suggest_int, suggest_categorical и suggest_float. Затем оно обучает модель на обучающей выборке и вычисляет\n",
        "# значение целевой функции, которая равна средней точности (average precision score) на обучающей выборке.\n",
        "# Средняя точность - это метрика качества обнаружения аномалий, которая вычисляется как площадь под кривой точность-полнота\n",
        "# (precision-recall curve). Чем выше средняя точность, тем лучше модель обнаруживает аномалии.\n",
        "# После проведения всех испытаний мы можем увидеть лучшие параметры и лучшее значение целевой функции,\n",
        "# которые были найдены в процессе оптимизации, а также два графика, которые показывают важность параметров\n",
        "# и параллельные координаты. Важность параметров показывает, насколько каждый параметр влияет на значение целевой функции,\n",
        "# а параллельные координаты показывают, как параметры и значение целевой функции меняются в разных испытаниях.\n",
        "# Эти графики помогут нам понять, какие параметры лучше всего подходят для нашей модели.\n",
        "\n",
        "optimize(objective, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053124c0",
      "metadata": {
        "id": "053124c0"
      },
      "source": [
        "Параметры могут менятся:\n",
        "Best params:{'use_ae': False, 'epochs': 175, 'dropout':0.13240843290940185, 'l2': 0.09164054973288507}\n",
        "Best value:0.5786104975205142\n",
        "Вот наши оптимальные параметры для модели Deep SVD, которая является одним из алгоритмов библиотеки PyOD.\n",
        "Пояснения:\n",
        "- use_ae - это булевый параметр, который определяет, использовать ли автоэнкодер для инициализации весов сети. Автоэнкодер - это нейронная сеть, которая учится сжимать и восстанавливать данные. Если use_ae равен False, то веса сети инициализируются случайным образом. В вашем случае use_ae равен False, что означает, что мы не используем автоэнкодер.\n",
        "- epochs - это количество эпох для обучения сети. Эпоха - это один проход по всем обучающим данным. Чем больше эпох, тем больше возможность сети научиться выделять аномалии. Однако слишком большое количество эпох может привести к переобучению, когда сеть запоминает обучающие данные и теряет способность обобщать на новых данных. В вашем случае epochs равен 175, что означает, что мы обучаем сеть 175 раз на обучающих данных.\n",
        "- dropout_rate - это параметр для регуляризации сети, который определяет долю нейронов, которые отключаются во время обучения. Регуляризация - это способ предотвратить переобучение, когда сеть становится слишком сложной и подстраивается под шум в данных. Dropout - это один из методов регуляризации, который случайным образом исключает некоторые нейроны из сети, заставляя ее учиться более устойчивым и разнообразным представлениям данных. В вашем случае dropout_rate равен 0.13, что означает, что мы отключим примерно 13% нейронов в каждом слое сети во время обучения.\n",
        "- l2_regularizer - это параметр для регуляризации сети, который определяет коэффициент L2-штрафа для весов сети. L2-штраф - это еще один метод регуляризации, который добавляет к функции потерь сети дополнительный член, пропорциональный квадрату нормы весов сети. Это заставляет сеть уменьшать величину весов и предотвращать их слишком большое возрастание. В вашем случае l2_regularizer равен 0.09, что означает, что вы накладываете средний L2-штраф на веса сети.\n",
        "\n",
        "- Best value - это метрика качества модели, которую мы выбралиранееи для оптимизации параметров. В вашем случае использована средняя точность (average precision score) как метрика качества модели.\n",
        "Средняя точность - это взвешенное среднее точностей, достигнутых при различных порогах, с увеличением полноты от предыдущего порога, используемого в качестве веса. Средняя точность лежит в диапазоне от 0 до 1, чем выше, тем лучше. В нашем случае best value равен 0.57, что означает, что мы достигли средней точности около 50%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa0b8b6b",
      "metadata": {
        "id": "fa0b8b6b"
      },
      "source": [
        "**Вывод**\n",
        "\n",
        "У меня возник вопрос - какие параметры лучше для обнаружения выбросов в данных:\n",
        "Deep SVD, ECOD или COPOD. Я думаю, это зависит от многих факторов, таких как тип и размер данных, степень зашумленности, желаемая скорость и точность и т.д.\n",
        "Я только могу предположить, некоторые критерии для сравнения этих алгоритмов.\n",
        "\n",
        "- Deep SVD - это алгоритм, основанный на глубоком одноклассовом методе опорных векторов, который использует нейронную сеть для обучения и предсказания. Он требует библиотеку TensorFlow и имеет много гиперпараметров, которые нужно настраивать. Он может давать хорошие результаты на сложных и высокоразмерных данных, но также может быть подвержен переобучению и требовать больше времени и ресурсов для обучения и инференса.\n",
        "Инференс - это процесс применения модели искусственного интеллекта (ИИ) к новым данным для получения предсказаний или решений.  Инференс отличается от обучения модели ИИ, которое заключается в настройке параметров модели на основе обучающих данных. Инференс обычно требует меньше вычислительных ресурсов, чем обучение, но все же может быть сложной и затратной задачей, особенно для больших и сложных моделей ИИ.\n",
        "- ECOD - это ансамбль для обнаружения выбросов на основе кластеризации, который использует несколько базовых детекторов, таких как K-Means, DBSCAN и OPTICS, для разбиения данных на кластеры и вычисления степени аномальности каждого кластера. Он не требует библиотеку TensorFlow и имеет меньше гиперпараметров, которые нужно настраивать. Он может давать хорошие результаты на средних и низкоразмерных данных, но также может быть чувствителен к выбору базовых детекторов и параметров кластеризации.\n",
        "- COPOD - это корреляционный метод обнаружения выбросов, который использует статистические свойства данных, такие как эмпирическая копула и скошенность, для оценки вероятности попадания каждого наблюдения в хвосты распределения. Он не требует библиотеку TensorFlow и не имеет гиперпараметров, которые нужно настраивать. Он может давать хорошие результаты на больших и разнородных данных, но также может быть нестабилен при наличии сильных выбросов или неоднородных распределений.\n",
        "\n",
        "В зависимости от наших целей и ограничений, мы можете выбрать тот алгоритм, который лучше подходит для задачи.\n",
        "\n",
        "Мы также можем сравнить их по различным метрикам качества, таким как средняя точность, ROC-кривая, F1-мера."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e5a9a9",
      "metadata": {
        "id": "75e5a9a9"
      },
      "source": [
        "Спасибо за внимание!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}